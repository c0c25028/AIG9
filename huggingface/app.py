# app_refactored.py

import os
import re
import time
import logging
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass, field
from dotenv import load_dotenv
import networkx as nx
import matplotlib
matplotlib.use('Agg')  # ã‚µãƒ¼ãƒãƒ¼ç’°å¢ƒã§å¿…é ˆ
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import google.generativeai as genai
from google.api_core.exceptions import GoogleAPIError, RetryError
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import gradio as gr
import io
from PIL import Image
import matplotlib.patches as mpatches
import matplotlib.lines as mlines

# --- Configuration & Setup ---
load_dotenv()  # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_env_api_keys() -> Dict[str, Optional[str]]:
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—"""
    return {
        'gemini_api_key': os.getenv('GEMINI_API_KEY'),
        'google_api_key': os.getenv('GOOGLE_API_KEY'),
        'google_cse_id': os.getenv('GOOGLE_CSE_ID')
    }


# --- Constants ---
class Config:
    MAX_LLM_RETRIES = 3
    NODE_SUMMARY_LENGTH = 25
    RETRY_DELAY_SECONDS = 5
    MAX_SEARCH_ITERATIONS = 5
    DEFAULT_MODEL_NAME = "gemini-2.5-flash-lite-preview-09-2025"
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    DEFAULT_SYSTEM_PROMPT = """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ã‚ãªãŸã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’æ®µéšçš„ã«èª¬æ˜ã—ãªãŒã‚‰å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã¯å¿…ãš `<think>` ã¨ `</think>` ã‚¿ã‚°ã§å›²ã‚“ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
å„æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã¯ä»¥ä¸‹ã®å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„:
ã‚¹ãƒ†ãƒƒãƒ—[ç•ªå·]: [ç¨®åˆ¥] å†…å®¹ï¼š[å…·ä½“çš„ãªæ€è€ƒå†…å®¹] (æ ¹æ‹ : ã‚¹ãƒ†ãƒƒãƒ—X, ã‚¹ãƒ†ãƒƒãƒ—Y)

åˆ©ç”¨å¯èƒ½ãªã‚¹ãƒ†ãƒƒãƒ—ç¨®åˆ¥ãƒªã‚¹ãƒˆ:
- å•é¡Œå®šç¾©
- ä»®èª¬æç¤º
- æƒ…å ±åé›† (å†…å®¹ã«ã¯ `[æ¤œç´¢ã‚¯ã‚¨ãƒªï¼šã“ã“ã«æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰]` ã®å½¢å¼ã§æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¨˜è¿°)
- æƒ…å ±åˆ†æ
- æ¤œè¨¼
- ä¸­é–“çµè«–
- è«–ç‚¹
- åè«–
- å‚ç…§ (å†…å®¹ã«ã¯å‚ç…§å…ƒã‚’è¨˜è¿°)
- æœ€çµ‚çµè«–å€™è£œ

æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¨˜è¿°ã—ãŸå¾Œã€`<think>` ã‚¿ã‚°ã®å¤–ã«æœ€çµ‚çš„ãªå›ç­”ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã®å†…å®¹ã¯ã€å¾Œã§è¦ç´„ã•ã‚Œã‚‹ã“ã¨ã‚’æ„è­˜ã—ã€å…·ä½“çš„ã‹ã¤ç°¡æ½”ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã€å„ã‚¹ãƒ†ãƒƒãƒ—ã¯15ã‚¹ãƒ†ãƒƒãƒ—ä»¥ä¸‹ã§è¨˜è¿°ã™ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã¯å‡ºåŠ›å½¢å¼ã®å³æ ¼ãªä¾‹ã§ã™ï¼š
è³ªå•ï¼šæ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ
<think>
ã‚¹ãƒ†ãƒƒãƒ—1: å•é¡Œå®šç¾© å†…å®¹ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æ—¥æœ¬ã®é¦–éƒ½ã«ã¤ã„ã¦è³ªå•ã—ã¦ã„ã‚‹ã€‚
ã‚¹ãƒ†ãƒƒãƒ—2: æƒ…å ±åé›† å†…å®¹ï¼š[æ¤œç´¢ã‚¯ã‚¨ãƒªï¼šæ—¥æœ¬ é¦–éƒ½]
ã‚¹ãƒ†ãƒƒãƒ—3: æƒ…å ±åˆ†æ å†…å®¹ï¼šæ¤œç´¢çµæœã«ã‚ˆã‚‹ã¨ã€æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã‚ã‚‹ã€‚ (æ ¹æ‹ : ã‚¹ãƒ†ãƒƒãƒ—2)
ã‚¹ãƒ†ãƒƒãƒ—4: æœ€çµ‚çµè«–å€™è£œ å†…å®¹ï¼šæ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã‚ã‚‹ã¨å›ç­”ã™ã‚‹ã€‚ (æ ¹æ‹ : ã‚¹ãƒ†ãƒƒãƒ—3)
</think>
æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚

---
ã“ã‚Œã¾ã§ã®æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã®å±¥æ­´ (ã‚‚ã—ã‚ã‚Œã°):
{previous_steps_str}
---
ç›´å‰ã®æ¤œç´¢çµæœ (å¿…è¦ãªå ´åˆã®ã¿å‚ç…§):
{search_results_str}
---

ä¸Šè¨˜ã®å±¥æ­´ã¨æ¤œç´¢çµæœã‚’è¸ã¾ãˆã€æ€è€ƒã‚’ç¶šã‘ã‚‹ã‹ã€æœ€çµ‚çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
æ€è€ƒã‚’ç¶šã‘ã‚‹å ´åˆã¯ã€æ–°ã—ã„`<think>`ãƒ–ãƒ­ãƒƒã‚¯ã§ã€ä»¥å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·ã¨ã¯é‡è¤‡ã—ãªã„ã‚ˆã†ã«ç¶šãã®ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã€ã‚ã„ã¾ã„ãªè³ªå•ã«ã¯è©³ç´°ã«æŒ‡å®šã™ã‚‹ã‚ˆã†ã«è¿”ç­”ã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š{user_question}"""


# --- Utility Functions ---
def setup_japanese_font_for_matplotlib() -> Optional[str]:
    """
    ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¢ã—ã€Matplotlibã«è¨­å®šã‚’è©¦ã¿ã¾ã™ã€‚
    Hugging Face Spacesç’°å¢ƒã‚’æƒ³å®šã—ã€'Noto Sans CJK JP'ã‚’å„ªå…ˆã—ã¾ã™ã€‚
    """
    font_candidates = [
        'Noto Sans CJK JP', 'IPAexGothic', 'Yu Gothic', 'Hiragino Sans', 'MS Gothic', 'Meiryo'
    ]
    
    try:
        if hasattr(fm, '_rebuild'):
            fm._rebuild()
    except Exception as e:
        logger.warning(f"Could not rebuild Matplotlib font cache. This may be normal. Error: {e}")

    available_fonts = {f.name for f in fm.fontManager.ttflist}
    for font_name in font_candidates:
        if font_name in available_fonts:
            try:
                plt.rcParams['font.family'] = font_name
                current_sans_serif = plt.rcParams.get('font.sans-serif', [])
                if 'DejaVu Sans' in current_sans_serif:
                    current_sans_serif.remove('DejaVu Sans')
                plt.rcParams['font.sans-serif'] = [font_name] + current_sans_serif
                logger.info(f"Found and set Japanese font for Matplotlib: '{font_name}'")
                return font_name
            except Exception as e:
                logger.debug(f"Font '{font_name}' found but failed to set: {e}")
    
    logger.warning("No suitable Japanese font found. Graph labels may not display correctly.")
    return None


INSTALLED_JAPANESE_FONT = setup_japanese_font_for_matplotlib()


# --- Data Classes ---
@dataclass
class ParsedStep:
    id: int
    type: str
    raw_content: str
    summarized_content: str = ""
    basis_ids: List[int] = field(default_factory=list)
    search_query: Optional[str] = None


@dataclass
class LLMThoughtProcess:
    raw_response: str
    steps: List[ParsedStep] = field(default_factory=list)
    final_answer: str = ""
    error_message: Optional[str] = None


# --- Core Classes ---
class GeminiHandler:
    """Gemini APIã¨ã®å¯¾è©±ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: str, model_name: str = Config.DEFAULT_MODEL_NAME, 
                 system_prompt: str = Config.DEFAULT_SYSTEM_PROMPT):
        if not api_key:
            raise ValueError("Gemini API key is not set.")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(
            model_name,
            generation_config={"temperature": 0.3},
        )
        self.model_name = model_name
        self.system_prompt = system_prompt
        self.error_messages: List[str] = []

    def update_system_prompt(self, new_prompt: str):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›´æ–°"""
        self.system_prompt = new_prompt
        logger.info("System prompt updated")

    def generate_response(self, user_question: str, previous_steps: Optional[List[ParsedStep]] = None, 
                         search_results: Optional[str] = None):
        """LLMã‹ã‚‰ã®å¿œç­”ã‚’ç”Ÿæˆ"""
        self.error_messages = []
        previous_steps_str = self._format_previous_steps(previous_steps) if previous_steps else "ãªã—"
        search_results_str = search_results if search_results else "ãªã—"
        
        prompt = self.system_prompt.format(
            user_question=user_question,
            previous_steps_str=previous_steps_str,
            search_results_str=search_results_str
        )

        for attempt in range(Config.MAX_LLM_RETRIES):
            try:
                logger.info(f"Sending request to Gemini (Attempt {attempt + 1}/{Config.MAX_LLM_RETRIES}) Model: {self.model_name}")
                response = self.model.generate_content(prompt)
                response_text = "".join(
                    part.text for part in response.candidates[0].content.parts 
                    if hasattr(part, 'text')
                ) if response.candidates else ""
                
                if self._validate_response_format(response_text):
                    yield response_text
                    return
                
                error_msg = f"Geminiã®å‡ºåŠ›å½¢å¼ã‚¨ãƒ©ãƒ¼ã€‚å†è©¦è¡Œä¸­({attempt + 1})ã€‚"
                self.error_messages.append(error_msg)
                logger.warning(f"Gemini response format error (Attempt {attempt + 1}). Retrying...")
                
                if attempt < Config.MAX_LLM_RETRIES - 1:
                    yield f"Geminiã®å‡ºåŠ›å½¢å¼ã‚¨ãƒ©ãƒ¼ã€‚å†è©¦è¡Œä¸­ã§ã™..."
                    time.sleep(Config.RETRY_DELAY_SECONDS)
                else:
                    yield "ERROR: LLM_FORMAT_ERROR_MAX_RETRIES"
                    return
                    
            except (GoogleAPIError, RetryError) as e:
                logger.error(f"Gemini API error (Attempt {attempt + 1}): {e}")
                self.error_messages.append(f"Gemini APIã‚¨ãƒ©ãƒ¼({e})ã€‚å†è©¦è¡Œä¸­...")
                
                if "API key not valid" in str(e):
                    yield "ERROR: API_KEY_INVALID"
                    return
                    
                if attempt < Config.MAX_LLM_RETRIES - 1:
                    yield f"Gemini APIã‚¨ãƒ©ãƒ¼ã€‚å†è©¦è¡Œä¸­ã§ã™..."
                    time.sleep(Config.RETRY_DELAY_SECONDS * (attempt + 2))
                else:
                    yield "ERROR: API_ERROR_MAX_RETRIES"
                    return
                    
            except Exception as e:
                logger.error(f"Unexpected error during Gemini API call: {e}")
                self.error_messages.append("äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ã€‚å†è©¦è¡Œä¸­...")
                
                if attempt < Config.MAX_LLM_RETRIES - 1:
                    yield "äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ã€‚å†è©¦è¡Œä¸­ã§ã™..."
                    time.sleep(Config.RETRY_DELAY_SECONDS)
                else:
                    yield "ERROR: UNEXPECTED_ERROR_MAX_RETRIES"
                    return
                    
        yield "ERROR: UNKNOWN_FAILURE_AFTER_RETRIES"

    def _format_previous_steps(self, steps: List[ParsedStep]) -> str:
        """å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ–‡å­—åˆ—å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not steps:
            return "ãªã—"
        
        return "\n".join([
            f"ã‚¹ãƒ†ãƒƒãƒ—{s.id}: {s.type} å†…å®¹ï¼š{s.raw_content}" + 
            (f" (æ ¹æ‹ : {', '.join(map(str, s.basis_ids))})" if s.basis_ids else "")
            for s in steps
        ])

    def _validate_response_format(self, response_text: str) -> bool:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã®æ¤œè¨¼"""
        if not response_text.strip():
            return False
        has_think_tags = "<think>" in response_text and "</think>" in response_text
        return has_think_tags

    def summarize_text(self, text_to_summarize: str, max_length: int = Config.NODE_SUMMARY_LENGTH) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„"""
        if not text_to_summarize or not text_to_summarize.strip():
            return "ï¼ˆç©ºã®å†…å®¹ï¼‰"
        
        try:
            prompt = f"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã§{max_length}æ–‡å­—ä»¥å†…ã‚’ç›®å®‰ã«è¦ç´„ã—ã¦ãã ã•ã„:\n\n\"{text_to_summarize}\""
            summary_model = genai.GenerativeModel(self.model_name)
            response = summary_model.generate_content(prompt)
            summary = "".join(
                part.text for part in response.candidates[0].content.parts 
                if hasattr(part, 'text')
            ).strip() if response.candidates else ""
            
            return summary if summary else text_to_summarize[:max_length] + "..."
        except Exception as e:
            raise e

    def get_error_messages(self) -> List[str]:
        """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—"""
        return self.error_messages


class GoogleSearchHandler:
    """Googleæ¤œç´¢APIã®ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: Optional[str], cse_id: Optional[str]):
        self.service = None
        self.is_enabled = False
        
        if not api_key or not cse_id:
            logger.warning("Google API key or CSE ID is not set. Search functionality will be disabled.")
            return
        
        try:
            self.service = build("customsearch", "v1", developerKey=api_key, cache_discovery=False)
            self.cse_id = cse_id
            self.is_enabled = True
            logger.info("Google Search Handler initialized.")
        except Exception as e:
            logger.error(f"Failed to initialize Google Search service: {e}")

    def search(self, query: str, num_results: int = 3) -> Optional[str]:
        """æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ"""
        if not self.is_enabled or not self.service:
            return "æ¤œç´¢æ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™ã€‚"
        
        try:
            res = self.service.cse().list(
                q=query, cx=self.cse_id, num=num_results, lr='lang_ja'
            ).execute()
            
            if 'items' not in res or not res['items']:
                return "æ¤œç´¢çµæœãªã—ã€‚"
            
            search_results_text = "æ¤œç´¢çµæœ:\n" + "\n\n".join([
                f"{i+1}. {item.get('title', '')}\n  {item.get('snippet', '').replace(chr(10), ' ')}"
                for i, item in enumerate(res['items'])
            ])
            
            return search_results_text.strip()
        except HttpError as e:
            return f"æ¤œç´¢APIã‚¨ãƒ©ãƒ¼: {e._get_reason()}"
        except Exception as e:
            return f"æ¤œç´¢ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}"


class ThoughtParser:
    """LLMå‡ºåŠ›ã®è§£æã‚¯ãƒ©ã‚¹"""
    
    def parse_llm_output(self, raw_llm_output: str) -> LLMThoughtProcess:
        """LLMã®å‡ºåŠ›ã‚’è§£æã—ã¦ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†è§£"""
        result = LLMThoughtProcess(raw_response=raw_llm_output)
        
        think_match = re.search(r"<think>(.*?)</think>", raw_llm_output, re.DOTALL)
        if not think_match:
            result.error_message = "LLMã®å¿œç­”ã«<think>ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            result.final_answer = raw_llm_output.strip()
            return result
        
        think_content = think_match.group(1).strip()
        result.final_answer = raw_llm_output[think_match.end():].strip()
        
        step_pattern = re.compile(
            r"ã‚¹ãƒ†ãƒƒãƒ—\s*(\d+)\s*:\s*([^å†…å®¹]+?)\s*å†…å®¹ï¼š(.*?)(?:\s*\(æ ¹æ‹ :\s*(?:ã‚¹ãƒ†ãƒƒãƒ—\s*\d+(?:,\s*ã‚¹ãƒ†ãƒƒãƒ—\s*\d+)*)\s*\))?$",
            re.MULTILINE | re.IGNORECASE
        )
        
        for match in step_pattern.finditer(think_content):
            try:
                raw_content_full = match.group(3).strip()
                basis_match = re.search(r'\(æ ¹æ‹ :\s*(.*?)\)', match.group(0), re.IGNORECASE)
                basis_ids = [
                    int(b_id) for b_id in re.findall(r'\d+', basis_match.group(1))
                ] if basis_match else []
                
                search_query_match = re.search(r"\[æ¤œç´¢ã‚¯ã‚¨ãƒªï¼š(.*?)\]", raw_content_full, re.IGNORECASE)
                search_query = search_query_match.group(1).strip() if search_query_match else None
                
                result.steps.append(ParsedStep(
                    id=int(match.group(1)),
                    type=match.group(2).strip(),
                    raw_content=raw_content_full,
                    basis_ids=basis_ids,
                    search_query=search_query
                ))
            except Exception as e:
                result.error_message = f"ã‚¹ãƒ†ãƒƒãƒ—è§£æã‚¨ãƒ©ãƒ¼: {match.group(0)} ({e})"
        
        return result


class GraphGenerator:
    """ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, llm_handler: GeminiHandler, installed_japanese_font: Optional[str]):
        self.llm_handler = llm_handler
        self.error_messages_for_graph: List[str] = []
        self.font_properties = fm.FontProperties(family=installed_japanese_font) if installed_japanese_font else None

    def _summarize_step_contents(self, steps: List[ParsedStep], progress_fn=None):
        """ã‚¹ãƒ†ãƒƒãƒ—å†…å®¹ã‚’è¦ç´„"""
        if not steps:
            return
        
        total_steps = len(steps)
        for i, step in enumerate(steps):
            if progress_fn:
                progress_fn((i + 1) / total_steps, f"ã‚¹ãƒ†ãƒƒãƒ— {step.id} è¦ç´„ä¸­ ({i+1}/{total_steps})...")
            
            if not step.summarized_content:
                try:
                    step.summarized_content = self.llm_handler.summarize_text(step.raw_content)
                except Exception as e:
                    self.error_messages_for_graph.append(f"S{step.id}è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}")
                    step.summarized_content = step.raw_content[:Config.NODE_SUMMARY_LENGTH] + "..."

    def _custom_hierarchical_layout(self, G, root_node=0, vertical_gap=0.8, horizontal_gap=1.5):
        """ã‚«ã‚¹ã‚¿ãƒ éšå±¤ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ"""
        if root_node not in G:
            return None
        
        levels, nodes_at_level = {root_node: 0}, {0: [root_node]}
        queue, visited = [root_node], {root_node}
        max_level = 0
        
        while queue:
            parent = queue.pop(0)
            children = sorted(list(G.successors(parent)))
            for child in children:
                if child not in visited:
                    visited.add(child)
                    levels[child] = levels[parent] + 1
                    if levels[child] not in nodes_at_level:
                        nodes_at_level[levels[child]] = []
                    nodes_at_level[levels[child]].append(child)
                    queue.append(child)
                    max_level = max(max_level, levels[child])
        
        pos = {}
        for level, nodes in nodes_at_level.items():
            level_width = (len(nodes) - 1) * horizontal_gap
            x_start = -level_width / 2
            for i, node in enumerate(sorted(nodes)):
                pos[node] = (x_start + i * horizontal_gap, -level * vertical_gap)
        
        return pos

    def create_thinking_graph(self, user_question: str, all_steps: List[ParsedStep], 
                            final_answer_text: str, progress_fn=None) -> Optional[plt.Figure]:
        """æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        self.error_messages_for_graph = []
        
        if self.llm_handler.get_error_messages():
            self.error_messages_for_graph.extend(self.llm_handler.get_error_messages())
        
        self._summarize_step_contents(all_steps, progress_fn)
        
        G, QUESTION_NODE_ID = nx.DiGraph(), 0
        
        # è³ªå•ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
        try:
            question_summary = self.llm_handler.summarize_text(user_question, 40)
            G.add_node(QUESTION_NODE_ID, label=f"è³ªå•:\n{question_summary}", 
                      type="question", color="skyblue")
        except Exception as e:
            self.error_messages_for_graph.append(f"è³ªå•è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}")
            G.add_node(QUESTION_NODE_ID, label=f"è³ªå•:\n{user_question[:40]}...", 
                      type="question", color="skyblue")
        
        valid_ids = {QUESTION_NODE_ID}
        prev_id = QUESTION_NODE_ID
        max_id = max((s.id for s in all_steps), default=0)
        
        # ã‚¹ãƒ†ãƒƒãƒ—ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
        for step in sorted(all_steps, key=lambda s: s.id):
            G.add_node(step.id, 
                      label=f"S{step.id}: {step.type}\n{step.summarized_content}",
                      type="ai_step", color="khaki")
            valid_ids.add(step.id)
            
            if G.has_node(prev_id):
                G.add_edge(prev_id, step.id, type="sequential")
            prev_id = step.id
        
        # æœ€çµ‚å›ç­”ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
        final_id = max_id + 1
        try:
            answer_summary = self.llm_handler.summarize_text(final_answer_text, 40)
            G.add_node(final_id, label=f"æœ€çµ‚å›ç­”:\n{answer_summary}", 
                      type="final_answer", color="lightgreen")
        except Exception as e:
            self.error_messages_for_graph.append(f"å›ç­”è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}")
            G.add_node(final_id, label=f"æœ€çµ‚å›ç­”:\n{final_answer_text[:40]}...", 
                      type="final_answer", color="lightgreen")
        
        if G.has_node(prev_id):
            G.add_edge(prev_id, final_id, type="sequential")
        
        # æ ¹æ‹ é–¢ä¿‚ã®ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        for step in all_steps:
            for basis_id in step.basis_ids:
                if basis_id in valid_ids and step.id in valid_ids:
                    G.add_edge(basis_id, step.id, type="basis")

        if not G.nodes():
            self.error_messages_for_graph.append("ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return None
        
        # ã‚°ãƒ©ãƒ•ã®æç”»
        fig, ax = plt.subplots(figsize=(
            min(20, max(12, G.number_of_nodes() * 2.2)),
            min(15, max(8, G.number_of_nodes() * 1.2))
        ))
        
        pos = self._custom_hierarchical_layout(G) or nx.spring_layout(G, seed=42)
        
        # ã‚¨ãƒƒã‚¸ã®æç”»
        for u, v, data in G.edges(data=True):
            start_pos, end_pos = pos[u], pos[v]
            style = 'arc3,rad=0.15' if data.get('type') == 'basis' else 'arc3,rad=0.05'
            arrow = mpatches.FancyArrowPatch(
                start_pos, end_pos,
                arrowstyle='-|>',
                mutation_scale=20,
                shrinkA=30, shrinkB=30,
                color="purple" if data.get('type') == 'basis' else "black",
                linestyle="dashed" if data.get('type') == 'basis' else "solid",
                connectionstyle=style,
                zorder=1
            )
            ax.add_patch(arrow)

        # ãƒãƒ¼ãƒ‰ã®æç”»
        font_dict = {'family': INSTALLED_JAPANESE_FONT, 'size': 8, 'weight': 'bold'}
        for node_id, node_data in G.nodes(data=True):
            x, y = pos[node_id]
            ax.text(
                x, y,
                s=node_data['label'],
                fontdict=font_dict,
                ha='center', va='center',
                bbox=dict(
                    boxstyle='round,pad=0.6',
                    facecolor=node_data['color'],
                    edgecolor='black',
                    linewidth=1,
                    alpha=0.9
                ),
                zorder=2
            )

        # å‡¡ä¾‹ã®æç”»
        handles = [
            mpatches.Patch(color='skyblue', label='è³ªå•'),
            mpatches.Patch(color='khaki', label='AIæ€è€ƒã‚¹ãƒ†ãƒƒãƒ—'),
            mpatches.Patch(color='lightgreen', label='æœ€çµ‚å›ç­”'),
            mlines.Line2D([], [], color='black', label='æ™‚ç³»åˆ—ã®æµã‚Œ'),
            mlines.Line2D([], [], color='purple', ls='--', label='æ ¹æ‹ ãƒ»å‚ç…§')
        ]
        ax.legend(handles=handles, loc='upper right', 
                 prop=self.font_properties, fontsize='small')
        
        ax.set_title("AIæ€è€ƒé€£é–ã®å¯è¦–åŒ– (Gemini)", 
                    fontproperties=self.font_properties, fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.axis('off')
        ax.margins(0.15)
        
        return fig

    def get_error_messages_html(self) -> str:
        """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’HTMLå½¢å¼ã§å–å¾—"""
        if not self.error_messages_for_graph:
            return ""
        
        error_list = ''.join(f'<li>{msg}</li>' for msg in self.error_messages_for_graph)
        return f"""<div style='color: red; border: 1px solid red; padding: 10px;'>
                   <strong>ã‚°ãƒ©ãƒ•æ³¨æ„:</strong><ul>{error_list}</ul></div>"""


class AISystem:
    """AIã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, gemini_api_key: str, google_api_key: Optional[str] = None, 
                 google_cse_id: Optional[str] = None, system_prompt: str = Config.DEFAULT_SYSTEM_PROMPT):
        if not gemini_api_key:
            raise ValueError("Gemini API key is required.")
        
        self.llm_handler = GeminiHandler(
            api_key=gemini_api_key, 
            system_prompt=system_prompt
        )
        self.search_handler = GoogleSearchHandler(
            api_key=google_api_key, 
            cse_id=google_cse_id
        )
        self.parser = ThoughtParser()
        self.graph_generator = GraphGenerator(
            llm_handler=self.llm_handler, 
            installed_japanese_font=INSTALLED_JAPANESE_FONT
        )

    def update_system_prompt(self, new_prompt: str):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›´æ–°"""
        self.llm_handler.update_system_prompt(new_prompt)

    def _append_and_renumber_steps(self, existing: List[ParsedStep], 
                                  new: List[ParsedStep]) -> List[ParsedStep]:
        """æ—¢å­˜ã®ã‚¹ãƒ†ãƒƒãƒ—ã«æ–°ã—ã„ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ ã—ã¦ãƒªãƒŠãƒ³ãƒãƒ¼"""
        updated = list(existing)
        existing_sigs = {(s.type, s.raw_content) for s in existing}
        max_id = max((s.id for s in existing), default=0)
        
        for step in new:
            if (step.type, step.raw_content) not in existing_sigs:
                max_id += 1
                step.id = max_id
                updated.append(step)
                existing_sigs.add((step.type, step.raw_content))
        
        return updated

    def process_question_iterations(self, user_question: str, progress_fn):
        """è³ªå•å‡¦ç†ã®åå¾©å‡¦ç†"""
        progress_fn(0, desc="æ€è€ƒé–‹å§‹...")
        steps, answer, raw_output, errors = [], "", "", []
        
        # æœ€åˆã®LLMå‘¼ã³å‡ºã—
        for update in self.llm_handler.generate_response(user_question):
            if "ERROR:" in update:
                raw_output = update
                break
            if update.endswith("..."):
                progress_fn(0.1, desc=update)
            else:
                raw_output = update
        
        errors.extend(self.llm_handler.get_error_messages())
        
        if "ERROR:" in raw_output:
            yield (f"AIå‡¦ç†ã‚¨ãƒ©ãƒ¼: {raw_output}", None, f"<p style='color:red;'>{raw_output}</p>")
            return
        
        # åˆå›è§£æ
        parsed = self.parser.parse_llm_output(raw_output)
        steps = self._append_and_renumber_steps(steps, parsed.steps)
        answer = parsed.final_answer
        
        if parsed.error_message:
            errors.append(f"ãƒ‘ãƒ¼ã‚µãƒ¼é€šçŸ¥: {parsed.error_message}")
        
        last_steps = parsed.steps
        
        # æ¤œç´¢åå¾©å‡¦ç†
        for i in range(Config.MAX_SEARCH_ITERATIONS):
            search_step = next((
                s for s in last_steps 
                if s.type == "æƒ…å ±åé›†" and s.search_query
            ), None)
            
            if not search_step:
                break
            
            progress_fn(0.3 + i*0.1, desc=f"æ¤œç´¢ä¸­({i+1})...")
            search_results = self.search_handler.search(search_step.search_query) or "æƒ…å ±å–å¾—å¤±æ•—"
            
            progress_fn(0.4 + i*0.1, desc="å†è€ƒä¸­...")
            for update in self.llm_handler.generate_response(user_question, steps, search_results):
                if "ERROR:" in update:
                    raw_output = update
                    break
                if update.endswith("..."):
                    progress_fn(0.5 + i*0.1, desc=update)
                else:
                    raw_output = update
            
            errors.extend(self.llm_handler.get_error_messages())
            
            if "ERROR:" in raw_output:
                errors.append(f"AIå‡¦ç†ã‚¨ãƒ©ãƒ¼(æ¤œç´¢å¾Œ): {raw_output}")
                break
            
            parsed = self.parser.parse_llm_output(raw_output)
            newly_parsed_steps = parsed.steps
            steps = self._append_and_renumber_steps(steps, newly_parsed_steps)
            
            if parsed.final_answer.strip():
                answer = parsed.final_answer
            
            if parsed.error_message:
                errors.append(f"ãƒ‘ãƒ¼ã‚µãƒ¼é€šçŸ¥(æ¤œç´¢å¾Œ): {parsed.error_message}")
            
            last_steps = newly_parsed_steps
        
        if not answer.strip():
            answer = "æœ€çµ‚å›ç­”ã®æ˜ç¤ºçš„å‡ºåŠ›ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚°ãƒ©ãƒ•ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"
        
        # ã‚°ãƒ©ãƒ•ç”Ÿæˆ
        progress_fn(0.8, desc="ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­...")
        summary_prog = lambda p, d: progress_fn(0.8 + p * 0.15, desc=d)
        fig = self.graph_generator.create_thinking_graph(user_question, steps, answer, summary_prog)
        
        graph_image = None
        if fig:
            try:
                buf = io.BytesIO()
                fig.savefig(buf, format='png', bbox_inches='tight', dpi=120)
                plt.close(fig)
                buf.seek(0)
                graph_image = Image.open(buf)
            except Exception as e:
                logger.error(f"Error converting graph to PIL Image: {e}")
                errors.append(f"ã‚°ãƒ©ãƒ•ç”»åƒã®å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")

        errors.extend(self.graph_generator.error_messages_for_graph)
        
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç”Ÿæˆ
        if errors:
            unique_errors = sorted(list(set(errors)))
            error_list = ''.join(f"<li>{msg}</li>" for msg in unique_errors)
            error_html = f"""<div style='color: red; border: 1px solid red; padding: 10px; margin-top:10px;'>
                            <strong>å‡¦ç†ã«é–¢ã™ã‚‹é€šçŸ¥:</strong><ul>{error_list}</ul></div>"""
        else:
            error_html = ""
        
        progress_fn(1.0, desc="å®Œäº†")
        yield (answer, graph_image, error_html)


def create_gradio_interface():
    """Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆ"""
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
    env_keys = get_env_api_keys()
    has_all_keys = all(env_keys.values())
    
    with gr.Blocks(theme=gr.themes.Soft(primary_hue="blue")) as demo:
        system_state = gr.State(None)
        
        gr.Markdown("# ğŸ§  AIæ€è€ƒé€£é–å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ  (Geminiç‰ˆ)")
        gr.Markdown("Gemini APIã‚’åˆ©ç”¨ã—ã¦ã€AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’æ®µéšçš„ã«å¯è¦–åŒ–ã—ã¾ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¾ã™ã€‚")
        # ã‚«ã‚¹ã‚¿ãƒ CSS: ã‚°ãƒ©ãƒ•ç”»åƒã‚’ç¢ºå®Ÿã«ä¸­å¤®ã«é…ç½®ï¼ˆflexãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’åˆ—ã«é©ç”¨ï¼‰
        gr.HTML("""
        <style>
        /* ã‚°ãƒ©ãƒ•åˆ—ã‚³ãƒ³ãƒ†ãƒŠã‚’ä¸­å¤®å¯„ã›ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã« */
        .graph-column { 
            display: flex !important; 
            flex-direction: column; 
            align-items: center; 
            justify-content: flex-start; 
            gap: 0.5rem; 
        }
        /* ç”»åƒ/ã‚­ãƒ£ãƒ³ãƒã‚¹è‡ªä½“ã‚’ä¸­å¤® & å¯å¤‰å¹… */
        .graph-column img, .graph-column canvas { 
            display: block !important; 
            margin: 0 auto !important; 
            max-width: 100% !important; 
            height: auto !important;
        }
        /* Imageã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå¤–æ ã‚’ãƒ•ãƒ«å¹…ã«ã—ã¤ã¤å†…éƒ¨ã‚’ä¸­å¤®å¯„ã› */
        #graph-image { 
            width: 100%; 
            text-align: center; 
        }
        #graph-image img { 
            max-width: 95%; 
        }
        /* ãƒ¢ãƒã‚¤ãƒ«å‘ã‘ã®ç¸®å°ä½™ç™½ */
        @media (max-width: 780px) { 
            #graph-image img { max-width: 100%; }
        }
        </style>
        """)
        
        # .envã«ã™ã¹ã¦ã®ã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã¯è¨­å®šç”»é¢ã‚’éè¡¨ç¤ºã«ã™ã‚‹
        with gr.Accordion("APIã‚­ãƒ¼è¨­å®š", open=not has_all_keys, visible=not has_all_keys) as api_accordion:
            with gr.Row():
                gemini_key_input = gr.Textbox(
                    label="Gemini API Key", 
                    type="password", 
                    placeholder="ã“ã“ã«Gemini APIã‚­ãƒ¼ã‚’å…¥åŠ›"
                )
                google_key_input = gr.Textbox(
                    label="Google Search API Key (ä»»æ„)", 
                    type="password", 
                    placeholder="æ¤œç´¢æ©Ÿèƒ½ã‚’ä½¿ã†å ´åˆã«å…¥åŠ›"
                )
                google_cse_id_input = gr.Textbox(
                    label="Google CSE ID (ä»»æ„)", 
                    type="text", 
                    placeholder="æ¤œç´¢æ©Ÿèƒ½ã‚’ä½¿ã†å ´åˆã«å…¥åŠ›"
                )
            
            set_api_key_button = gr.Button("APIã‚­ãƒ¼ã‚’è¨­å®š", variant="primary")
            api_status_output = gr.Markdown()
        
        # .envã‹ã‚‰ã‚­ãƒ¼ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®è¡¨ç¤º
        if has_all_keys:
            gr.Markdown("âœ… APIã‚­ãƒ¼ãŒç’°å¢ƒå¤‰æ•°ã‹ã‚‰è‡ªå‹•èª­ã¿è¾¼ã¿ã•ã‚Œã¾ã—ãŸ")
        
        with gr.Accordion("ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š", open=False):
            system_prompt_input = gr.Textbox(
                label="ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
                value=Config.DEFAULT_SYSTEM_PROMPT,
                lines=30,
                max_lines=30,
                placeholder="AIã®å‹•ä½œã‚’åˆ¶å¾¡ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
                info="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã§ {user_question}, {previous_steps_str}, {search_results_str} ã‚’ä½¿ç”¨ã§ãã¾ã™"
            )
            update_prompt_button = gr.Button("ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›´æ–°", variant="secondary")
            prompt_status_output = gr.Markdown()
        
        # .envã«ã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ãƒ¡ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æœ€åˆã‹ã‚‰è¡¨ç¤º
        with gr.Column(visible=has_all_keys) as main_interface:
            gr.Markdown("---")
            question_input = gr.Textbox(
                label="è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", 
                placeholder="ä¾‹: æ—¥æœ¬ã®AIæŠ€è¡“ã®æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰ã¨ãã®èª²é¡Œã¯ä½•ã§ã™ã‹ï¼Ÿ", 
                lines=10
            )
            submit_button = gr.Button("è³ªå•ã‚’é€ä¿¡ã™ã‚‹", variant="primary")
            
            gr.Markdown("## ğŸ¤– AIã®å›ç­”ã¨æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹")
            with gr.Row():
                with gr.Column(scale=1):
                    answer_output = gr.Markdown(label="AIã®æœ€çµ‚å›ç­”")
                    graph_errors_output = gr.HTML(label="å‡¦ç†ã«é–¢ã™ã‚‹é€šçŸ¥")
                with gr.Column(scale=2, elem_classes=["graph-column"]):
                    graph_output = gr.Image(
                        label="æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³", 
                        type="pil", 
                        interactive=False, 
                        show_download_button=True,
                        elem_id="graph-image"
                    )
            
            gr.Examples(
                examples=[
                    ["æ—¥æœ¬ã®AIæŠ€è¡“ã®æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰ã¨ãã®ç¤¾ä¼šã¸ã®å½±éŸ¿ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"],
                    ["å¤ªé™½å…‰ç™ºé›»ã®ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’æ•´ç†ã—ã€ä»Šå¾Œã®å±•æœ›ã‚’äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚"]
                ], 
                inputs=[question_input]
            )

        def set_api_keys(gemini_key, google_key, cse_id, system_prompt):
            """APIã‚­ãƒ¼ã‚’è¨­å®š"""
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚­ãƒ¼ã‚’å–å¾—ã—ã¦ã€å…¥åŠ›ã•ã‚ŒãŸã‚­ãƒ¼ã¨çµ±åˆ
            env_keys = get_env_api_keys()
            
            final_gemini_key = gemini_key.strip() if gemini_key and gemini_key.strip() else env_keys['gemini_api_key']
            final_google_key = google_key.strip() if google_key and google_key.strip() else env_keys['google_api_key']
            final_cse_id = cse_id.strip() if cse_id and cse_id.strip() else env_keys['google_cse_id']
            
            if not final_gemini_key:
                return (
                    None, 
                    gr.update(value="<p style='color:red;'>Gemini APIã‚­ãƒ¼ã¯å¿…é ˆã§ã™ã€‚</p>"), 
                    gr.update(visible=False)
                )
            
            try:
                system = AISystem(
                    gemini_api_key=final_gemini_key, 
                    google_api_key=final_google_key, 
                    google_cse_id=final_cse_id,
                    system_prompt=system_prompt
                )
                search_status = "æœ‰åŠ¹" if system.search_handler.is_enabled else "ç„¡åŠ¹"
                key_source = "ç’°å¢ƒå¤‰æ•°" if not gemini_key.strip() else "æ‰‹å‹•å…¥åŠ›"
                status_md = f"""<p style='color:green;'>APIã‚­ãƒ¼è¨­å®šå®Œäº†ã€‚è³ªå•ã‚’é–‹å§‹ã§ãã¾ã™ã€‚</p>
                               <p><strong>Googleæ¤œç´¢</strong>: `{search_status}`</p>
                               <p><strong>APIã‚­ãƒ¼å–å¾—å…ƒ</strong>: `{key_source}`</p>"""
                
                return system, gr.update(value=status_md), gr.update(visible=True)
            except Exception as e:
                logger.error(f"Failed to initialize AI system: {e}")
                return (
                    None, 
                    gr.update(value=f"<p style='color:red;'>ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}</p>"), 
                    gr.update(visible=False)
                )

        def update_system_prompt(system, new_prompt):
            """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›´æ–°"""
            if system is None:
                return gr.update(value="<p style='color:red;'>å…ˆã«APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚</p>")
            
            if not new_prompt.strip():
                return gr.update(value="<p style='color:orange;'>ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç©ºã§ã™ã€‚</p>")
            
            try:
                system.update_system_prompt(new_prompt)
                return gr.update(value="<p style='color:green;'>ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸã€‚</p>")
            except Exception as e:
                logger.error(f"Failed to update system prompt: {e}")
                return gr.update(value=f"<p style='color:red;'>ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}</p>")

        def submit_fn_wrapper(system, question, progress=gr.Progress(track_tqdm=True)):
            """è³ªå•é€ä¿¡ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°"""
            if system is None:
                return (
                    "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", 
                    None, 
                    "<p style='color:red;'>ã¾ãšAPIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚</p>"
                )
            
            if not question.strip():
                return (
                    "è³ªå•ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", 
                    None, 
                    "<p style='color:orange;'>è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚</p>"
                )
            
            final_result = (None, None, None)
            for result in system.process_question_iterations(question, progress):
                final_result = result
            return final_result

        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¨­å®š
        set_api_key_button.click(
            fn=set_api_keys,
            inputs=[gemini_key_input, google_key_input, google_cse_id_input, system_prompt_input],
            outputs=[system_state, api_status_output, main_interface]
        )
        
        update_prompt_button.click(
            fn=update_system_prompt,
            inputs=[system_state, system_prompt_input],
            outputs=[prompt_status_output]
        )
        
        submit_button.click(
            fn=submit_fn_wrapper,
            inputs=[system_state, question_input],
            outputs=[answer_output, graph_output, graph_errors_output]
        )
        
        # ã‚¨ãƒ³ã‚¿ãƒ¼ã‚­ãƒ¼ã§é€ä¿¡ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ 
        question_input.submit(
            fn=submit_fn_wrapper,
            inputs=[system_state, question_input],
            outputs=[answer_output, graph_output, graph_errors_output]
        )
        
        # .envã«ã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã¯èµ·å‹•æ™‚ã«è‡ªå‹•è¨­å®š
        if has_all_keys:
            def auto_init():
                system = AISystem(
                    gemini_api_key=env_keys['gemini_api_key'],
                    google_api_key=env_keys['google_api_key'], 
                    google_cse_id=env_keys['google_cse_id'],
                    system_prompt=Config.DEFAULT_SYSTEM_PROMPT
                )
                return system
            
            demo.load(
                fn=auto_init,
                inputs=[],
                outputs=[system_state]
            )
    
    return demo


if __name__ == "__main__":
    gradio_app = create_gradio_interface()
    gradio_app.launch()